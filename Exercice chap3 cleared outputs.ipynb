{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EXERCICE CHAPITRE 3 #####\n",
    "### PARTIE 1 ###\n",
    "\n",
    "#1: Take the dict created in the TODO 4 in chapter I and save it in the collection \"CRUD_exercise\".\n",
    "\n",
    "import pymongo\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"TODO\"]\n",
    "collection = db[\"CRUB_exercise\"]\n",
    "\n",
    "lecun_paper = {\n",
    "    \"title\": \"Deep Learning\",\n",
    "    \"authors\": {\n",
    "        \"Yann LeCun\": {\"affiliations\": [\"Facebook AI Research\", \"New York University\"]},\n",
    "        \"Yoshua Bengio\": {\"affiliations\": [\"Department of Computer Science and Operations Research Université de Montréal\"]},\n",
    "        \"Geoffrey Hinton\": {\"affiliations\": [\"Google\", \"Department of Computer Science, University of Toronto\"]}\n",
    "    }\n",
    "}\n",
    "goodfellow_paper = {\n",
    "    \"title\": \"Generative Adversarial Nets\",\n",
    "    \"authors\": {\n",
    "        \"Ian Goodfellow\": {\"affiliations\": [\"Universite de Montreal\"]},\n",
    "        \"Jean Pouget-Abadie\": {\"affiliations\": [\"Ecole Polytechnique\"]},\n",
    "        \"Mehdi Mirza\": {\"affiliations\": [\"Université de Montréal\"]},\n",
    "        \"Bing Xu\": {\"affiliations\": [\"Université de Montréal\"]},\n",
    "        \"David Warde-Farley\": {\"affiliations\": [\"Université de Montréal\"]},\n",
    "        \"Sherjil Ozair\": {\"affiliations\": [\"Indian Institute of Technology Delhi\"]},\n",
    "        \"Aaron Courville\": {\"affiliations\": [\"Université de Montréal\"]},\n",
    "        \"Yoshua Bengio\": {\"affiliations\": [\"CIFAR Senior Fellow\"]}\n",
    "    }\n",
    "}\n",
    "papers_dict = {\n",
    "    \"LeCun et al.\": lecun_paper,\n",
    "    \"Goodfellow et al.\": goodfellow_paper\n",
    "}\n",
    "\n",
    "collection.insert_one(papers_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 : Insert 3 documents with key = x and values = 1, delete one of them. Which one is deleted first ? the most recent or oldest one ? increment the value of x to 4.\n",
    "\n",
    "for i in range(1, 4):\n",
    "    collection.insert_one({\"key\": \"x\", \"value\": 1})\n",
    "\n",
    "# Afficher les documents avant la suppression\n",
    "print(\"Documents avant la suppression :\")\n",
    "for doc in collection.find({\"key\": \"x\"}):\n",
    "    print(doc)\n",
    "\n",
    "# Supprimer un des documents\n",
    "collection.delete_one({\"key\": \"x\"})\n",
    "\n",
    "# Afficher les documents après la suppression\n",
    "print(\"\\nDocuments après la suppression :\")\n",
    "for doc in collection.find({\"key\": \"x\"}):\n",
    "    print(doc)\n",
    "\n",
    "\n",
    "docs = list(collection.find({\"key\": \"x\"}).sort(\"_id\", pymongo.ASCENDING))\n",
    "if len(docs) == 2:\n",
    "    if docs[0][\"_id\"] < docs[1][\"_id\"]:\n",
    "        print(\"\\nLe document supprimé était le plus ancien.\")\n",
    "    else:\n",
    "        print(\"\\nLe document supprimé était le plus récent.\")\n",
    "else:\n",
    "    print(\"\\nIl y a eu une erreur dans la suppression des documents.\")\n",
    "\n",
    "collection.update_many({\"key\": \"x\"}, {\"$set\": {\"value\": 4}})\n",
    "\n",
    "print(\"\\nDocuments après l'incrémentation de la valeur de x :\")\n",
    "for doc in collection.find({\"key\": \"x\"}):\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le document supprimé est le plus ancien "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3: Insert the dict created in the TODO 6 Chapter I in the example collection.\n",
    "\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"tutorial\"]\n",
    "collection = db[\"example\"]\n",
    "\n",
    "# Load data from the JSON file\n",
    "with open('xml_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Insert the data into the collection\n",
    "collection.insert_one(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 : Get documents where authors key exist in the collection \"CRUD_exercise\".\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"TODO\"]\n",
    "collection = db[\"CRUD_exercise\"]\n",
    "\n",
    "documents_with_authors = collection.find({\"authors\": {\"$exists\": True}})\n",
    "print(documents_with_authors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 : Change the documents where x = 4 to x = 1.\n",
    "\n",
    "collection.update_many({\"x\": 4}, {\"$set\": {\"x\": 1}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 : Find documents where author is not_mike and set author as real_mike.\n",
    "collection.update_many({\"author\": \"not_mike\"}, {\"$set\": {\"author\": \"real_mike\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 : Delete documents where author is real_mike\n",
    "collection.delete_many({\"author\": \"real_mike\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARTIE 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 : create a collection named \"CRUD_exercise_benchmark\" with 500k observations, ids increment of 2 (sequence:0,2,4,6,...1M). Give a random np.array with a key named \"values\" and use the insert_many. Then create an index on the id and benchmark queries before and after indexing. Did the index help ?\n",
    "\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('localhost')\n",
    "mydb = client[\"TODO\"]\n",
    "collection = mydb[\"CRUD_exercise_benchmark\"]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "values = np.random.rand(500000, 4)\n",
    "docs = [{\"id\": i, \"values\": values[i//2].tolist()} for i in range(0, 1000000, 2)]\n",
    "\n",
    "collection.insert_many(docs)\n",
    "\n",
    "import time\n",
    "\n",
    "# No index\n",
    "start_time = time.time()\n",
    "docs = collection.find({\"id\": 100000})\n",
    "end_time = time.time()\n",
    "print(\"Temps sans index: \", end_time - start_time)\n",
    "\n",
    "# Index\n",
    "start_time = time.time()\n",
    "docs = collection.find({\"id\": 100000}).hint(\"id_1\")\n",
    "end_time = time.time()\n",
    "print(\"Temps avec index: \", end_time - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9 : \n",
    "import pymongo\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "random_db = client[\"random_db\"]\n",
    "random_collection = random_db[\"random_collection\"]\n",
    "\n",
    "doc = {\"key\": \"value\"}\n",
    "random_collection.insert_one(doc)\n",
    "\n",
    "tutorial_db = client[\"tutorial\"]\n",
    "tutorial_collection = tutorial_db[\"tutorial_collection\"]\n",
    "tutorial_collection.insert_many(random_collection.find())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 : \n",
    "# Inner join returns only the matching documents from both collections whereas outer join returns all documents \n",
    "# from both collections, including non-matching documents as null. The query seen during the course was an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real World Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11: Use the oaipmh and api code get papers after January 2020 and for \"cs,math,econ\" categories. Insert them in MongoDB. Import only the first 200. \n",
    "#How is it sorted ? How can you define your own sort()? Query papers to get papers after 2021, which have 3 authors and with domain \"cs\".\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "import tqdm\n",
    "import time\n",
    "import pymongo\n",
    "\n",
    "# For each id get all the metadata https://info.arxiv.org/help/api/basics.html#python_simple_example\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "mydb = client[\"tutorial\"]\n",
    "collection = mydb[\"arxiv_api\"]\n",
    "\n",
    "# get list of ids previously downloaded\n",
    "with open(\"arxiv_cs.txt\",\"r\") as lines:\n",
    "    ids = list(set(lines.read().split(\"\\n\")[0:-2]))\n",
    "\n",
    "#init list of ids and iteration\n",
    "ids_query = []\n",
    "\n",
    "# loop through ids\n",
    "for id_ in tqdm.tqdm(ids):\n",
    "    #append id to list\n",
    "    ids_query.append(id_)\n",
    "    # if len list = 100 \n",
    "    if len(ids_query) == 100 :\n",
    "        # collapse list of id\n",
    "        ids_query = \",\".join(ids_query)\n",
    "        # query the api for the 100 ids\n",
    "        response = requests.get('http://export.arxiv.org/api/query?id_list={}&max_results=100'.format(ids_query))\n",
    "        # parse response\n",
    "        feed = feedparser.parse(response.content)\n",
    "        # commit the 100 papers found\n",
    "        list_of_insertion = []\n",
    "        for entry in feed.entries:\n",
    "            list_of_insertion.append(dict(entry))\n",
    "        collection.insert_many(list_of_insertion)\n",
    "        ids_query = []\n",
    "        time.sleep(1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = collection.find({\n",
    "    \"published_parsed\": {\"$gte\": \"2021-01-01T00:00:00Z\"},\n",
    "    \"$where\": \"this.authors.length === 3\",\n",
    "    \"arxiv_primary_category.term\": \"cs\"\n",
    "})\n",
    "\n",
    "for paper in papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#12: Do the same as exercise 8 but with the connection to the cluster. Then check the metrics and take screenshot of opcounters, logical size and connections.\n",
    "collection.find( { \"id\": \"http://arxiv.org/abs/2401.01379v1\"} ).explain()['executionStats']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.create_index([(\"id\", 1)])\n",
    "collection.find( { \"id\": \"http://arxiv.org/abs/2401.01379v1\"} ).explain()['executionStats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Opcounters:\", client.admin.command(\"serverStatus\")[\"opcounters\"])\n",
    "print(\"Taille logique:\", client.admin.command(\"collstats\", \"arxiv_api\")[\"size\"])\n",
    "print(\"Connexions:\", client.admin.command(\"serverStatus\")[\"connections\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13 : Download a random image and store it in a collection.\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "image = Image.open('primedelhumour.png')\n",
    "pyplot.imshow(image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#14: Try to store a pandas dataframe in mongoDB (array with rownames, array with colnames and matrix with values)\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "mydb = client[\"TODO\"]\n",
    "collection = mydb[\"dftest\"]\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Insert the DataFrame into MongoDB\n",
    "collection.insert_one(df.to_dict(orient='split'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15: Insert the movie_review.tsv data into mongodb. Then query it to find the number of review that are positive and negative review. Fetch the docs which have \"unexpected\" in their review, how many are they ? Think of a clever way to count the number of words in the review using MongoDB (hint: Transform the review text before the insert in MongoDB) and create a density of number of words per review.\n",
    "df=pd.read_csv(\"movie_review.tsv\", sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "db = client['TODO']  \n",
    "collection = db['movies'] \n",
    "\n",
    "# Convert the \"Phrase\" column to lowercase and remove punctuation\n",
    "df['Phrase'] = df['Phrase'].str.lower()\n",
    "df['Phrase'] = df['Phrase'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Insert data into MongoDB\n",
    "data = df.to_dict(orient='records')\n",
    "collection.insert_many(data)\n",
    "\n",
    "# Count the number of positive and negative reviews\n",
    "positive_reviews_count = collection.count_documents({\"Sentiment\": 1})\n",
    "negative_reviews_count = collection.count_documents({\"Sentiment\": 2})\n",
    "\n",
    "print(\"Number of positive reviews:\", positive_reviews_count)\n",
    "print(\"Number of negative reviews:\", negative_reviews_count)\n",
    "\n",
    "# Get documents containing the word \"unexpected\" in their reviews\n",
    "unexpected_reviews_count = collection.count_documents({\"Phrase\": {\"$regex\": \"unexpected\", \"$options\": \"i\"}})\n",
    "\n",
    "print(\"Number of reviews containing 'unexpected':\", unexpected_reviews_count)\n",
    "\n",
    "# Calculate word density per review\n",
    "pipeline = [\n",
    "    {\"$project\": {\"_id\": 0, \"Phrase\": 1}},  \n",
    "    {\"$addFields\": {\"word_count\": {\"$size\": {\"$split\": [\"$Phrase\", \" \"]}}}},  \n",
    "    {\"$group\": {\"_id\": None, \"total_word_count\": {\"$sum\": \"$word_count\"}, \"total_reviews\": {\"$sum\": 1}}},  \n",
    "    {\"$project\": {\"_id\": 0, \"word_density\": {\"$divide\": [\"$total_word_count\", \"$total_reviews\"]}}}  \n",
    "]\n",
    "\n",
    "result = list(collection.aggregate(pipeline))\n",
    "\n",
    "if result:\n",
    "    word_density = result[0]['word_density']\n",
    "    print(\"Word density per review:\", word_density)\n",
    "else:\n",
    "    print(\"No documents found in the collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real World Application 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index, explain your choice of key.\n",
    "\n",
    "import pymongo\n",
    "import json\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client[\"TODO\"]\n",
    "collection = db[\"pubmed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pubmed_cleaned.json', \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for doc in data:\n",
    "    if '_id' in doc:\n",
    "        doc['_id'] = str(doc['_id']['$oid'])\n",
    "\n",
    "try:\n",
    "    collection.insert_many(data)\n",
    "except pymongo.errors.BulkWriteError as e:\n",
    "    print(\"Bulk write error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''key '_id' conctionne en primary key et est une bonne solution pour identifier chaque articles dans la base de données.'''\n",
    "\n",
    "collection.create_index([('_id',1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Delete every paper that was published prior 2019 \n",
    "from datetime import datetime\n",
    "docs = collection.find()\n",
    "for doc in docs:\n",
    "    date_string = doc['date']\n",
    "    date_string = date_string.replace(\"year\", \"\").replace(\"month\", \"\").replace(\"day\", \"\").replace(\"hour\", \"\").replace(\"minute\", \"\").replace(\",\",\"-\").replace(\" \",\"\")\n",
    "    date = datetime.strptime(date_string, \"%Y-%m-%d-%H-%M\")\n",
    "    if date.year < 2019:\n",
    "        collection.delete_one({'_id':doc['_id']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) How many paper have a single author ? Two authors ?\n",
    "single_author_count = collection.count_documents({\"$expr\": {\"$eq\": [{\"$size\": {\"$split\": [\"$authors\", \"\\n\"]},}, 1]}})\n",
    "two_authors_count = collection.count_documents({\"$expr\": {\"$eq\": [{\"$size\": {\"$split\": [\"$authors\", \"\\n\"]},}, 2]}})\n",
    "print(\"Number of papers with a single author: \", single_author_count)\n",
    "print(\"Number of papers with two authors: \", two_authors_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) What's the last paper inserted in the db ?\n",
    "last_paper = collection.find().sort([(\"_id\", -1)]).limit(1)\n",
    "print(\"Last paper inserted in the db: \", last_paper[0]['pmid'], last_paper[0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Find articles with null meshwords.\n",
    "docs = collection.find({\"meshwords\": None})\n",
    "for doc in docs:\n",
    "    print(doc['pmid'], doc['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Choose a keyword you are interested in (machine learning, computer vision,...). \n",
    "# Find the number of articles with the choosen keyword in their meshwords, abstract or title.\n",
    "keyword = \"machine learning\"\n",
    "docs = collection.find({\"$or\": [{\"meshwords\": {\"$regex\": keyword, \"$options\": \"i\"}},\n",
    "                                 {\"abstract\": {\"$regex\": keyword, \"$options\": \"i\"}}, \n",
    "                                 {\"title\": {\"$regex\": keyword, \"$options\": \"i\"}}]})\n",
    "doc_list = list(docs)\n",
    "doc_num = len(doc_list)\n",
    "print(doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) What's the number of articles that have at least one affiliation AND meshwords.\n",
    "docs = collection.find({\"$and\": [{\"affiliation\": {\"$ne\": \"\"}}, \n",
    "                                 {\"meshwords\": {\"$ne\": \"\"}}]\n",
    "                                 })\n",
    "doc_list = list(docs)\n",
    "doc_num = len(doc_list)\n",
    "print(\"Le nombre d'article qui ont au moins une affiliation et des mots-clés est :\", doc_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. How many articles have a publishing date after 2020 ?\n",
    "\n",
    "docs = collection.find({\"date\": {\"$gt\": datetime(2020, 1, 1)}})\n",
    "doc_count = len(list(docs))\n",
    "print(doc_count, \"article on été publié en 2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Find articles where there's atleast one affiliation from a choosen country (you decide which one).\n",
    "country = \"Germany\"\n",
    "docs = collection.find({\"authors\": {\"$regex\": country, \"$options\": \"i\"}})\n",
    "doc_list = list(docs)\n",
    "doc_num = len(doc_list)\n",
    "print(\"Nombre d'articles avec au moins une affiliation en allemagne:\", doc_num)\n",
    "\n",
    "for doc in doc_list:\n",
    "    print(doc['title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Check for any duplicates. (hint: look at the doi or the pmid)\n",
    "pipline = [{\"$group\": {\"_id\": {\"doi\": \"$doi\"}, \"count\": {\"$sum\": 1}}}, \n",
    "           {\"$match\": {\"count\": {\"$gt\": 1}}}]\n",
    "\n",
    "docs = collection.aggregate(pipline)\n",
    "for doc in docs:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Remove every articles where the abstract starts with an \"R\".\n",
    "result = collection.delete_many({\"abstract\": {'$regex': \"^R\", \"$options\": \"i\"}})\n",
    "deleted_count = result.deleted_count\n",
    "print(\"Nombre d'articles supprimés:\", deleted_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Return the list of papers (pmid) where there's atleast one affiliation per author\n",
    "docs = collection.find({\"authors\": {\"$regex\": \"affil str\"}})\n",
    "pmid_list = [doc['pmid'] for doc in docs]\n",
    "\n",
    "print(\"Liste des articles où chaque auteur a au moins une affiliation:\", pmid_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13. Create 500 random samples of the dataset, compute a statistics that you are interested in and check how it behaves through the different samples\n",
    "import random\n",
    "\n",
    "num_samples = 500\n",
    "sample_size = 100\n",
    "sample_statistics = []\n",
    "\n",
    "\n",
    "def compute_statistic(sample_data):\n",
    "    return sum(sample_data) / len(sample_data)\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    docs = collection.find({})\n",
    "    random_sample = random.sample(list(docs), sample_size)\n",
    "    \n",
    "    sample_data = [doc[\"nb_country\"] for doc in random_sample]\n",
    "    statistic = compute_statistic(sample_data)\n",
    "    sample_statistics.append(statistic)\n",
    "\n",
    "# Pour une meilleur vu dans l'ensemble on va faire un histogramme.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(sample_statistics, bins=20, edgecolor='black')\n",
    "plt.xlabel('Valeurs de la statistique')\n",
    "plt.ylabel('cumule du noumbre de pays')\n",
    "plt.title('Distribution du nombre de pays à travers les échantillons aléatoires')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Sandbox exercise: think of a problematic and try to answer it : How many articles have a publishing date after 2020 in france ?\n",
    "\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "\n",
    "doc_count = collection.count_documents({\"$and\": [{\"date\": {\"$gt\": datetime(2019, 1, 1)}}, {\"country\": \"France\"}]})\n",
    "\n",
    "# Affichage du nombre d'articles\n",
    "print(doc_count, \"articles ont été publiés après 2019 en France.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real World Application 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import json\n",
    "\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "db = client[\"TODO\"]\n",
    "collection = db[\"authors\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('authors.json', \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for doc in data:\n",
    "    if '_id' in doc:\n",
    "        doc['_id'] = str(doc['_id']['$oid'])\n",
    "\n",
    "collection.insert_many(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create an index, explain your choice of key.\n",
    "''' _id est une valeur unique pour chaque document, c'est plus facile de faire les recherches par _id.'''\n",
    "\n",
    "collection.create_index([(\"_id\",1) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. What is the average length of \"pmid_list\"\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$pmid_list\"},\n",
    "    {\"$group\": {\"_id\": None, \"avg_length\": \n",
    "                {\"$avg\": \n",
    "                 {\"$strLenCP\": \n",
    "                  {\"$toString\": \"$pmid_list\"}}}}}] # d'abord on transforme pmid_list en string et après on calcule la longueur\n",
    "\n",
    "result = list(collection.aggregate(pipeline))\n",
    "print(result[0]['avg_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. How many distinct affiliations are there ?\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$oa04_affiliations\"}, \n",
    "    {\"$group\": {\"_id\": \"$oa04_affiliations.Affiliation\"}},  # group by affiliation\n",
    "    {\"$count\": \"num_unique_affiliations\"} \n",
    "]\n",
    "\n",
    "result = list(collection.aggregate(pipeline))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Find authors with atleast one \"COM\" AffiliationType\n",
    "\n",
    "pipeline = [{\"$unwind\": \"$oa04_affiliations\"},\n",
    "            {\"$match\": {\"oa04_affiliations.AffiliationType\": \"COM\"}},\n",
    "            {\"$project\": {\"_id\": 1, \"name\": 1, \"oa04_affiliations\": 1}}]\n",
    "\n",
    "result = list(collection.aggregate(pipeline))\n",
    "\n",
    "authors_com = []\n",
    "for author in result:\n",
    "    authors_com.append(author['_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.How many authors switched the AffiliationType ?\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$oa04_affiliations\"},\n",
    "    {\"$group\": {\"_id\": \"$_id\", \"num_types\": {\"$addToSet\": \"$oa04_affiliations.AffiliationType\"}}},\n",
    "    {\"$project\": {\"_id\": 1, \"num_types\": {\"$size\": \"$num_types\"}}},\n",
    "    {\"$match\": {\"num_types\": {\"$gt\": 1}}},\n",
    "    {\"$count\": \"num_authors\"}]\n",
    "\n",
    "result = list(collection.aggregate(pipeline))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Find affiliation with the word \"China\"\n",
    "\n",
    "pipeline = [{\"$unwind\": \"$oa04_affiliations\"},\n",
    "    {\"$match\": {\"oa04_affiliations.Affiliation\": {\"$regex\": \"China\"}}},\n",
    "    {\"$project\": {\"_id\": 1, \"name\": 1, \"oa04_affiliations.Affiliation\": 1}}]\n",
    "\n",
    "result = list(collection.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Get the pmids of papers published in 2019\n",
    "\n",
    "pipeline = [\n",
    "    {\"$unwind\": \"$more_info\"},\n",
    "    {\"$match\": {\"more_info.PubYear\": 2019}},\n",
    "    {\"$project\": {\"_id\": 1, \"name\": 1, \"more_info.PMID\": 1}}]\n",
    "\n",
    "result = list(collection.aggregate(pipeline))\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Count the number of doc with \"oa06_researcher_education\" OR \"oa04_affiliations\" key and \n",
    "# with the \"oa06_researcher_education\" AND \"oa04_affiliations\" .\n",
    "\n",
    "pipeline1 = [{\"$match\": {\"$or\": [{\"oa06_researcher_education\": {\"$exists\": True}}, \n",
    "                                {\"oa04_affiliations\": {\"$exists\": True}}]}}]\n",
    "\n",
    "pipeline2 =[{\"$match\": {\"$and\": [{\"oa06_researcher_education\": {\"$exists\": True}},\n",
    "                                {\"oa04_affiliations\": {\"$exists\": True}}]}}]\n",
    "\n",
    "                               \n",
    "result1= list(collection.aggregate(pipeline1))\n",
    "print(len(result1))\n",
    "\n",
    "result2= list(collection.aggregate(pipeline2))\n",
    "print(len(result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. What's the average \"BeginYear\", \"BeginYear\" is the type string, of \"oa06_researcher_education\".\n",
    "pipeline = [{\"$unwind\": \"$oa06_researcher_education\"},\n",
    "    {\"$addFields\": {\"BeginYear\": {\"$cond\": {\"if\": {\"$eq\": [\"$oa06_researcher_education.BeginYear\",' ']}, \n",
    "                                            \"then\":None, \"else\": {\"$toInt\": \"$oa06_researcher_education.BeginYear\"}}}}},\n",
    "    {\"$match\": {\"BeginYear\": {\"$ne\":None}}},\n",
    "    {\"$group\": {\"_id\": None, \"avg_BeginYear\": {\"$avg\": \"$BeginYear\"}}}]\n",
    "\n",
    "result = list(collection.aggregate(pipeline))\n",
    "print(result[0]['avg_BeginYear'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.Count the distinct country of \"oa06_researcher_education\"\n",
    "\n",
    "pipeline = [{\"$unwind\": \"$oa06_researcher_education\"},\n",
    "    {\"$group\": {\"_id\": \"$oa06_researcher_education.Country\"}},\n",
    "    {\"$count\": \"num_unique_countries\"}]\n",
    "\n",
    "result = list(collection.aggregate(pipeline))\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
